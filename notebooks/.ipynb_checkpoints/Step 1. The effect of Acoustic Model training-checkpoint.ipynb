{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The effect of Acoustic Model training\n",
    "In this notebook we evaluate the effect of `Acoustic Model training`. We walk you through the process of comparing a baseline pre-trained acoustic model with an acoustic model finetuned on domain specific data.\n",
    "\n",
    "1. We first train an acoustic model on a domain specific dataset. More specifically, we finetune a pre-trained baseline model, where we perform knowledge transfer from a [model trained on LibriSpeech with EN Common Voice](https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5) and train it with new a domain specific dataset.\n",
    "\n",
    "2. We compare the performance of the baseline model and the fine-tuned model by performing inference with both models and compare their WER performance. \n",
    "\n",
    "To simplify and enable reproducibility of the ASR workflow, this application allows you to create a `project` which enables the tracking of datasets, models and configurations across all workflows â€“ i.e. acoustic model and language model pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "os.environ['APP_DIR']='..'\n",
    "os.environ['DATA_DIR']=os.path.join(os.environ['APP_DIR'],'data')\n",
    "sys.path.append(os.environ['APP_DIR'])\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join('..', os.path.dirname(os.path.dirname('.'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print (\"GPU is enabled:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from easydict import EasyDict as edict\n",
    "import torchaudio\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import tarfile\n",
    "import wget\n",
    "import requests\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.display\n",
    "import re\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "from tools.System.config import cfg\n",
    "from tools.System.reader import Reader\n",
    "from tools.misc import parse_manifest_wer, barplot_manifest, get_transcript, get_gtruth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create a new project\n",
    "\n",
    "We create a project that will keep track of the datasets, configurations and models created across the acoustic model and language model workflows.\n",
    "Everything related to a project is saved in disc in a manifest that can be access through its `project_id`.\n",
    "\n",
    "At the start of the project, the manifest is pre-populated with the baseline pretrained models. For acoustic model we use a pretrained model on LibriSpeech with a greedy decoder and for language model we use a Baidu's CTC decoder model. Both pre-trained models were built or downloaded in `Step 0` and thus exist in the `example_data` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT = 'http://www.openslr.org/resources/7/'\n",
    "DATA_PREFIX = 'TEDLIUM'\n",
    "DATASET_NAME = f\"{DATA_PREFIX}_release1.tar.gz\"\n",
    "\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "\n",
    "\n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT,'data')\n",
    "current_datasets_dir = os.path.join(DATA_DIR,DATA_PREFIX)\n",
    "dataset_path = os.path.join(current_datasets_dir,DATASET_NAME)\n",
    "\n",
    "print('Dataset path:', dataset_path)\n",
    "\n",
    "if not os.path.exists(dataset_path):\n",
    "    print(f\"Downloading {DATASET_NAME}: {ENDPOINT+DATASET_NAME}\")\n",
    "    \n",
    "    if not os.path.exists(current_datasets_dir):\n",
    "        os.mkdir(current_datasets_dir)\n",
    "    \n",
    "    download_url(ENDPOINT+DATASET_NAME, dataset_path)\n",
    "    \n",
    "else:\n",
    "    print(\"Tarfile already exists.\")\n",
    "    \n",
    "if not os.path.exists(dataset_path[:-7]):\n",
    "    print(\"Decompressing data\")\n",
    "    tar = tarfile.open(dataset_path)\n",
    "    tar.extractall(current_datasets_dir)\n",
    "else:\n",
    "    print(\"Tarfile has been already decompressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build manifest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### librispeech manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librispeech manifest\n",
    "\n",
    "def libri_speech_chapter_build_manifest(transcripts_path, manifest_path, flac_path):\n",
    "    with open(transcripts_path, 'r') as fin:\n",
    "        with open(manifest_path, 'w') as fout:\n",
    "            for line in fin:\n",
    "                transcript = ' '.join(line.split(' ')[1:]).lower()\n",
    "                file_id = line.split(' ')[0]\n",
    "                audio_path = os.path.join(\n",
    "                     flac_path,\n",
    "                     file_id + '.flac')\n",
    "                y, sr = sf.read(audio_path)\n",
    "                duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "                # Write the metadata to the manifest\n",
    "                metadata = {\n",
    "                     \"audio_filepath\": audio_path,\n",
    "                     \"duration\": duration,\n",
    "                     \"text\": transcript\n",
    "                }\n",
    "                \n",
    "                json.dump(metadata, fout)\n",
    "                fout.write('\\n')\n",
    "\n",
    "\n",
    "TRAIN_DATA_DIR = Path(os.path.join(current_datasets_dir,'LibriSpeech/train-clean-100'))\n",
    "train_texts = list(TRAIN_DATA_DIR.glob('**/*.trans.txt'))\n",
    "train_audios = list(TRAIN_DATA_DIR.glob('**/*.flac'))\n",
    "\n",
    "def generate_manifest(_path, list_of_train_texts):\n",
    "    for the_train_transcripts in list_of_train_texts:\n",
    "        libri_speech_chapter_build_manifest(the_train_transcripts , _path, the_train_transcripts.parent)\n",
    "\n",
    "train_manifest_path = os.path.join(current_datasets_dir, 'train_manifest.json')\n",
    "test_manifest_path = os.path.join(current_datasets_dir, 'test_manifest.json')\n",
    "\n",
    "\n",
    "generate_manifest(_path=train_manifest_path,list_of_train_texts=train_texts[:40])\n",
    "generate_manifest(_path=test_manifest_path,list_of_train_texts=train_texts[40:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### abstract manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_abstract_manifest(manifest_path, audio_files: list, transcripts: list):\n",
    "    \"\"\"\n",
    "    audio_files: list of paths to audios, \n",
    "    transcripts: list of strings of the same length as audio_files\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(manifest_path, 'w') as fout\n",
    "        for audio_path, transcript in zip(audio_files, transcripts)\n",
    "        y, sr = sf.read(audio_path)\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "        current_line = {\n",
    "             \"audio_filepath\": audio_path,\n",
    "             \"duration\": duration,\n",
    "             \"text\": transcript\n",
    "        }\n",
    "\n",
    "        json.dump(metadata, fout)\n",
    "        fout.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from tools.sph2wav import SPHFile\n",
    "\n",
    "class AbstractDataTransformer(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_audio_and_transcrpits(self, path, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "class TedliumDataTransformer(AbstractDataTransformer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, sample_rate):\n",
    "        self.sample_rate: int = sample_rate\n",
    "    \n",
    "    def get_audio_and_transcrpits(self, path):\n",
    "        \n",
    "        transformed_audio_path = os.path.join(path, 'transformed_audio')\n",
    "        \n",
    "        if not os.path.exists(transformed_audio_path):\n",
    "            os.mkdir(transformed_audio_path)\n",
    "                \n",
    "        transformed_data = list()\n",
    "        \n",
    "        audio_dir = os.path.join(path,'sph')\n",
    "        transripts_dir = os.path.join(path,'stm')\n",
    "        \n",
    "        for audio_path, transcript_path in zip(os.listdir(audio_dir),\n",
    "                                  os.listdir(transripts_dir)):\n",
    "            \n",
    "            sph_audio = SPHFile(os.path.join(audio_dir, audio_path)).open()\n",
    "            text_segments = self.get_segments_from_transcript(os.path.join(transripts_dir,transcript_path))\n",
    "            audio_segments = self.get_audio_segments(os.path.join(audio_dir,audio_path), text_segments)\n",
    "            \n",
    "            for idx, audio_segment in enumerate(audio_segments):\n",
    "                audio_segment_path = os.path.join(transformed_audio_path, audio_path[:-4])\n",
    "                if not os.path.exists(audio_segment_path):\n",
    "                    os.mkdir(audio_segment_path)\n",
    "                audio_segment_chunk_path = os.path.join(audio_segment_path, f'{str(idx)}.wav')\n",
    "\n",
    "                sph_audio.write_wav(audio_segment_chunk_path, audio_segment['start'], audio_segment['end'])\n",
    "                transformed_data.append({'transcript':audio_segment['transcript'],\n",
    "                                         'audio':audio_segment_chunk_path})\n",
    "        return transformed_data\n",
    "            \n",
    "            \n",
    "    def get_segments_from_transcript(self, transcript_path):\n",
    "        with open(transcript_path) as transcript_fd:\n",
    "            transcripts = transcript_fd.read()\n",
    "            segments = [ent for ent in transcripts.split('\\n') \n",
    "                        if '' != ent]\n",
    "        return segments\n",
    "\n",
    "    def get_audio_segments(self, audio_path, text_segments):\n",
    "        _segments = list()\n",
    "        for idx, segment in enumerate(text_segments):\n",
    "            the_md, the_transcript = re.split(r' <.+> ',segment)\n",
    "            if the_transcript.strip() == 'ignore_time_segment_in_scoring':\n",
    "                continue\n",
    "            left_timestamp, right_timestamp = map(float,the_md.split(' ')[-2:])\n",
    "            left_idx = left_timestamp\n",
    "            right_idx = right_timestamp\n",
    "            _segments.append({'audio_path':audio_path,\n",
    "                              'start': left_idx,\n",
    "                              'end': right_idx,\n",
    "                              'transcript':the_transcript})\n",
    "        return _segments\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedlium_transformer = TedliumDataTransformer(sample_rate=16)\n",
    "tedlium_dev_dataset_path = os.path.join(dataset_path[:-7], 'dev')\n",
    "data = tedlium_transformer.get_audio_and_transcrpits(path=tedlium_dev_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_audio(path_to_audio):\n",
    "    audio, sample_rate = sf.read(path_to_audio)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (15,7)\n",
    "    plt.title(f'Waveform of Audio Example: {path_to_audio}')\n",
    "    plt.ylabel('Amplitude')\n",
    "\n",
    "    _ = librosa.display.waveplot(audio)\n",
    "    \n",
    "    return audio, sample_rate\n",
    "\n",
    "example_transcript = data[0]['transcript']\n",
    "example_audio = data[0]['audio']\n",
    "\n",
    "audio, sample_rate = visualize_audio(example_audio)\n",
    "print(example_transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuningExecutor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.experiment_name: str = None\n",
    "        \n",
    "        #train manifest and working directory for logs\n",
    "        self.train_manifest_path: str = None            \n",
    "        self.workdir: str = None\n",
    "        \n",
    "        self.jasper_train_script_path: str = None\n",
    "            \n",
    "        #paths to external configs   \n",
    "        self.model_architecture_path: str = None\n",
    "        self.pretrained_encoder_path: str = None\n",
    "        self.pretrained_decoder_path: str = None\n",
    "        \n",
    "        #params\n",
    "        self.batch_size: int = None\n",
    "        self.num_epochs: int = None\n",
    "        self.learning_rate: float = None        \n",
    "        self.optimizer: str = None\n",
    "        \n",
    "        self.nproc_per_node: int = None\n",
    "        self.amp_opt_level: str = None\n",
    "        \n",
    "    def generate_train_command(self):\n",
    "        command = f\"\"\"!python3 -m torch.distributed.launch \n",
    "                    --nproc_per_node={self.nproc_per_node} {self.jasper_train_script_path} \n",
    "                    --batch_size={self.batch_size}\n",
    "                    --num_epochs={self.num_epochs}\n",
    "                    --lr={self.learning_rate} \n",
    "                    --model_config={self.model_architecture_path}  \n",
    "                    --optimizer={self.optimizer}\n",
    "                    --amp_opt_level={self.amp_opt_level}\n",
    "                    --finetune \n",
    "                    --load_encoder={self.pretrained_encoder_path} \n",
    "                    --load_decoder={self.pretrained_decoder_path}  \n",
    "                    --work_dir={self.workdir} \n",
    "                    --exp_name={self.experiment_name} \n",
    "                    --train_dataset={self.train_manifest_path} \n",
    "                  \"\"\"\n",
    "        command = command.replace('\\n',' ').strip()\n",
    "        command = re.sub(r'\\s{2,}',r' ',command)\n",
    "        return command\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"FineTuningExedcutor for experiment {self.experiment_name}\"\n",
    "    \n",
    "    \n",
    "finetuner = FineTuningExecutor()\n",
    "finetuner.experiment_name = DATA_PREFIX\n",
    "finetuner.train_manifest_path = os.path.join(current_datasets_dir, train_manifest_path)\n",
    "finetuner.workdir = os.path.join(DATA_DIR,f'model_{DATA_PREFIX}')\n",
    "finetuner.jasper_train_script_path = os.path.join(PROJECT_ROOT,'tools/NeMo/jasper_train.py')\n",
    "finetuner.model_architecture_path = os.path.join(PROJECT_ROOT,'demo/quartznet15x5/quartznet15x5.yaml')\n",
    "finetuner.pretrained_encoder_path = os.path.join(PROJECT_ROOT,'demo/quartznet15x5/JasperEncoder-STEP-247400.pt')\n",
    "finetuner.pretrained_decoder_path = os.path.join(PROJECT_ROOT,'demo/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt')\n",
    "finetuner.batch_size = 16\n",
    "finetuner.num_epochs=5\n",
    "finetuner.learning_rate=0.0001\n",
    "finetuner.optimizer='novograd'\n",
    "finetuner.amp_opt_level='O1'\n",
    "finetuner.nproc_per_node=torch.cuda.device_count()\n",
    "\n",
    "finetuner.generate_train_command()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m torch.distributed.launch --nproc_per_node=1 /home/ubuntu/gong/nemo_domain_specific/tools/NeMo/jasper_train.py --batch_size=16 --num_epochs=5 --lr=0.0001 --model_config=/home/ubuntu/gong/nemo_domain_specific/demo/quartznet15x5/quartznet15x5.yaml --optimizer=novograd --amp_opt_level=O1 --finetune --load_encoder=/home/ubuntu/gong/nemo_domain_specific/demo/quartznet15x5/JasperEncoder-STEP-247400.pt --load_decoder=/home/ubuntu/gong/nemo_domain_specific/demo/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt --work_dir=/home/ubuntu/gong/nemo_domain_specific/data/model_librispeech --exp_name=librispeech --train_dataset=/home/ubuntu/gong/nemo_domain_specific/data/librispeech/train_manifest.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine-tune Pretrained Acoustic Model with `Domain Specific` Data\n",
    "Next, we finetune a baseline model with our own data.\n",
    "\n",
    "The steps are: \n",
    "1. Add pre-processed dataset to project\n",
    "2. Adjust training parameters\n",
    "3. Generate training command\n",
    "4. Perform Training \n",
    "\n",
    "Note, if the WSJ is not available to you, you can alternatively walk through these 4 steps using a dummy dataset but in the process the system will generate the correct directories. \n",
    "Then you can replace the checkpoints with the checkpoints of the **fine-tuned WSJ model** used to generate the results below, available in NGC in this [link](https://ngc.nvidia.com/models/nvidia:wsj_quartznet_15x5) \n",
    "You will place these checkpoints in the following directory:\n",
    "`[data_dir]/models/acoustic_models/WSJ/WSJ_finetuning-lr_0.0001-bs_16-e_100-wd_0.0-opt_novograd/checkpoints/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Add Training Datasets to the Project \n",
    "To train an acoustic model you require a pre-processed `json` training dataset.\n",
    "NeMo expects the dataset as a `json` file where each entry has the keys: `audio_filepath`, `duration` and `text`.\n",
    "\n",
    "Note: Make sure the pre-processed dataset has the correct paths to the audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "train_json=os.path.join(cfg.DATASET.PATHS.EXAMPLE_DATA, \"wsj\", \"wsj-train-si284-speed-0.9-1.1.json\")\n",
    "\n",
    "project.add_dataset(train_json, \"WSJ-wsj-train-speed\", dataset_type=\"am-train\")\n",
    "project.manifest.am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Modify acoustic model training parameters\n",
    "The default training parameters are specified in the project manifest, training script and the configuration file  `[DATA_DIR]/config_files/[Project_id]_acoustic_quartznet15x5.yaml`. However, you can modify some of these parameters as follows.\n",
    "\n",
    "To see the full list of parameters accessible by the manifest see `project.manifest.am.train_params`, to see other parameters look at the NeMo training script at `/tools/NeMo/jasper_train.py`.\n",
    "\n",
    "Important Notes:\n",
    "- The parameter `amp_opt_level` set to O1 or above, enables NVIDIA's [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision).\n",
    "- To enable finetuning you must set the parameter `finetuning` to \"True\" and set the path to the `pre-trained encoder and decoder` to the parameters `load_encoder` and `load_decoder` respectively. Note, the finetuning and pre-trained encoder and decoders are already added at the start of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.set_am_batch_size(16)\n",
    "project.set_am_num_gpus(4)\n",
    "project.set_am_learning_rate(0.0001)\n",
    "project.manifest.am.train_params.warmup_steps = 0 \n",
    "#project.manifest.am.train_params.weight_decay = 0.0001\n",
    "\n",
    "# If your dataset has 200+ Hrs of audio you can fine-tune for less epochs, e.g. 100.\n",
    "project.set_am_num_epochs(100)\n",
    "\n",
    "# novograd\n",
    "#project.manifest.am.train_params.beta1 = 0.95\n",
    "#project.manifest.am.train_params.beta2 = 0.25 \n",
    "\n",
    "# To modify the path to config file and the pretrained model\n",
    "#project.load_am_config_file(config_file='')\n",
    "#project.set_am_pretrained_model(model='')\n",
    "\n",
    "project.save_manifest()\n",
    "project.manifest.am"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Generate Training Command\n",
    "Next we generate the acoustic training command, we use the settings specified in the manifest to create the appropriate command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_training_cmd = project.get_am_train_cmd()\n",
    "print(\"! \"+ am_training_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Fine-tune Pretrained Model\n",
    "Now that we have the training command that reflects the project configuration, we can perform acoustic model training. Note, acoustic model training can take a long time so it is be better to run training directly from the terminal of the container, using the command generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, if you completed the training before, you'll have checkpoints saved in the working directory.  We'll need to remove these to restart training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean log directory to re-run the training command\n",
    "#project.clean_am_workdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy (select cmd + shift RMB + copy) the command generated and paste it below to run training inside the notebook, we recommend you run it at the container's terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m torch.distributed.launch --nproc_per_node=4 /home/adrianaf/projects/asr_system/nemo_asr_app/tools/NeMo/jasper_train.py --batch_size=16 --num_epochs=100 --lr=0.0001 --model_config=/raid/datasets/asr/data/config_files/WSJ_acoustic_quartznet15x5.yaml --optimizer=novograd --amp_opt_level=O1 --finetune --load_encoder=/home/adrianaf/projects/asr_system/nemo_asr_app/demo/quartznet15x5/JasperEncoder-STEP-247400.pt --load_decoder=/home/adrianaf/projects/asr_system/nemo_asr_app/demo/quartznet15x5/JasperDecoderForCTC-STEP-247400.pt --work_dir=/raid/datasets/asr/data/models/acoustic_models/WSJ --exp_name=WSJ_finetuning --train_dataset=/raid/datasets/asr/data/example_data/wsj/wsj-train-si284-speed-0.9-1.1.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inference with Pre-trained and Fine-tuned Models\n",
    "Next, we perform inference with both the pre-trained and fine-tuned models.\n",
    "\n",
    "The steps are: \n",
    "1. Add pre-processed evaluation datasets to project\n",
    "2. Inference with Pre-trained model\n",
    "3. Inference with Fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Add Inference Evaluation Datasets to the Project \n",
    "To perform inference you require a preprocessed `json` dataset, with the columns: `audio_filepath`, `duration` and `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add inference dataset\n",
    "eval_json=os.path.join(cfg.DATASET.PATHS.EXAMPLE_DATA, \"wsj\",\"wsj-eval-92.json\")\n",
    "#eval_json=os.path.join(cfg.DATASET.PATHS.EXAMPLE_DATA, \"wsj\",\"wsj-dev-93.json\")\n",
    "\n",
    "project.add_dataset(eval_json, \"wsj-eval\", dataset_type=\"eval\")\n",
    "\n",
    "print(project.manifest.eval_dataset_names)\n",
    "print(project.manifest.eval_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Inference with Pre-trained model\n",
    "To do inference, we follow 3 steps: 1) Adjust inference parameters. 2) Generate inference command 3) Perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Modify inference parameters\n",
    "The default inference parameters are specified in the project manifest, inference script and the configuration file. However, you can modify some of these parameters as follows.\n",
    "\n",
    "To see the full list of parameters accessible by the manifest see `project.manifest.inference_params`, to see other parameters look at the NeMo inference script at `/tools/NeMo/jasper_eval.py`.\n",
    "\n",
    "Important Notes:\n",
    "- The parameter `amp_opt_level` set to O1 or above, enables NVIDIA's [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision).\n",
    "\n",
    "- When `lm_path` is set to \"None\", the system will use a `greedy decoder`, and thus the language model parametrs, e.g. `beam_width`, `alpha`, `beta` will not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.manifest.inference_params.batch_size = 32\n",
    "project.manifest.inference_params.amp_opt_level = 'O0'\n",
    "\n",
    "project.save_manifest()\n",
    "project.manifest.inference_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Generate Inference Command: Pre-trained Model\n",
    "We generate the inference command using the manifest configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Id - We use this to identify the results of this model\n",
    "model_id = \"am-pretrained-greedy-decoder\"\n",
    "\n",
    "# Inference with Pre-trained model\n",
    "am_infer_cmd = project.get_inference_cmd(model_id=model_id)\n",
    "print(\"! \"+ am_infer_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Run Inference with Pre-trained Model\n",
    "To run inference, copy (select cmd + shift RMB + copy) the command generated above and paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /home/adrianaf/projects/asr_system/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/home/adrianaf/projects/asr_system/nemo_asr_app/demo/quartznet15x5 --model_config=/raid/datasets/asr/data/config_files/WSJ_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O0 --save_results=/raid/datasets/asr/data/inferences/WSJ --eval_datasets=/raid/datasets/asr/data/example_data/wsj/wsj-eval-92.json --model_id=am-pretrained-greedy-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Inference with Finetuned model\n",
    "To do inference with our finetuned model, we have to modify the manifest parameters to point to our finetuned model, then follow the same process as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Modify inference parameters\n",
    "The default inference parameters are specified in the project manifest, inference script and the configuration file. However, you can modify some of these parameters as follows.\n",
    "\n",
    "To see the full list of parameters accessible by the manifest see `project.manifest.inference_params`, to see other parameters look at the NeMo inference script at `/tools/NeMo/jasper_eval.py`.\n",
    "\n",
    "Important Notes:\n",
    "- The parameter `amp_opt_level` set to O1 or above, enables NVIDIA's [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision).\n",
    "\n",
    "- When `lm_path` is set to \"None\", the system will use a `greedy decoder`, and thus the language model parametrs, e.g. `beam_width`, `alpha`, `beta` will not be used.\n",
    "\n",
    "\n",
    "Next, you **must** modify the `project.manifest.inference_params.load_dir` to use a different model than the default pretrained model.\n",
    "\n",
    "- The path to the finetuned model can be found at `project.manifest.am.finetuned_model_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the finetuned model\n",
    "project.manifest.am.finetuned_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference with Fintuned model\n",
    "# We provide the finetuned model inside the demo folder, but if you trained the model the path is available in project.manifest.am.finetuned_model_path\n",
    "project.manifest.inference_params.load_dir = os.path.join(os.environ['APP_DIR'], 'demo', 'finetuned') \n",
    "project.save_manifest()\n",
    "project.manifest.inference_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Generate Inference Command\n",
    "We generate the inference command using the manifest configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Id - We use this to identify the results of this model\n",
    "model_id = \"am-finetuned-WSJ_train_speed-greedy-decoder\"\n",
    "\n",
    "# Inference with Pretrained model\n",
    "am_infer_cmd = project.get_inference_cmd(model_id=model_id)\n",
    "print(\"! \"+ am_infer_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Run Inference with Fine-tuned Model\n",
    "To run inference, copy (select cmd + shift RMB + copy) the command generated above and paste below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python /home/adrianaf/projects/asr_system/nemo_asr_app/tools/NeMo/jasper_eval.py --load_dir=/home/adrianaf/projects/asr_system/nemo_asr_app/demo/finetuned --model_config=/raid/datasets/asr/data/config_files/WSJ_acoustic_quartznet15x5.yaml --batch_size=32 --amp_opt_level=O0 --save_results=/raid/datasets/asr/data/inferences/WSJ --eval_datasets=/raid/datasets/asr/data/example_data/wsj/wsj-eval-92.json --model_id=am-finetuned-WSJ_train_speed-greedy-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Results and Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Add Inference results to manifest\n",
    "We first add the results from our inference runs to the project's manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.add_inference_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the inference results are added, we can see them in the manifest. \n",
    "The results are structured as follows: For each evaluation dataset, we have a multiple models identified by their `model_id` and inside those we have the model's results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_datasets = list(project.manifest.inference.keys())\n",
    "print(\"Datasets with inference: \", inf_datasets)\n",
    "\n",
    "for inf in inf_datasets:\n",
    "    print()\n",
    "    print(inf)\n",
    "    print(project.manifest.inference[inf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the `Transcript` and `Ground Truth` from an evaluation dataset, you can use the commands `get_transcript` and `get_gtruth` using the path to the inference results which can be obtained by `project.get_inf_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to get the path to the inference results - you need the dataset path and the model-id you wish to retrieve\n",
    "path = project.get_inf_path('/raid/datasets/asr/data/example_data/wsj/wsj-eval-92.json',\n",
    "                            'am-finetuned-WSJ_train_speed-greedy-decoder')\n",
    "print(path)\n",
    "# get transcript and ground truth\n",
    "transcript = get_transcript(path)\n",
    "gtruth = get_gtruth(path)\n",
    "print(transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Model Comparison\n",
    "Next, we compare the Word Error Rates for both the fine-tuned model and pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferences_all = list(project.manifest.inference.keys())\n",
    "inferences_all.sort()\n",
    "print(inferences_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a eval dataset to plot its performance and we also select the performance metric we wish to sort by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_inference = project.manifest.inference[inferences_all[0]]\n",
    "sort_metric = 'percentWER'\n",
    "\n",
    "# Note if you wish to plot (and parse) only a specific set of models you can use the argument 'keep' and \n",
    "# list the model_id(s) you wish to keep.\n",
    "am_df = parse_manifest_wer(sel_inference, sort_metric=sort_metric, keep=['am-finetuned-WSJ_train_speed-greedy-decoder',\n",
    "                                                                         'am-pretrained-greedy-decoder'])\n",
    "print(inferences_all[0])\n",
    "am_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the dataframe created by the `parse_manifest_wer` function to plot the performance of the different models.\n",
    "We use the function `barplot_manifest` that has the arguments `([dataframe],[metric to plot],[title],[xlabel],[ylabel])`.\n",
    "\n",
    "Note:\n",
    "WER percent may be above 100% when the number of words in the prediction is larger than the number of words in the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "am_wer = barplot_manifest(am_df,'percentWER',\"WSJ - Acoustic Model - Word Error Rate\", \"Inference type\",\"Percent %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. WER effect of Acoustic training\n",
    "As shown in the plot, finetuning the acoustic model on our domain data can greatly improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "am_df.set_index('inference_types', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org = am_df.loc['am-pretrained-greedy-decoder','wer'] # pre-trained model\n",
    "new = am_df.loc['am-finetuned-WSJ_train_speed-greedy-decoder','wer'] # fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_decrease = 100*((org-new)/org)\n",
    "print(\"Decreased WER by {0:.2f}%\".format(per_decrease))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
